{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e009d9c-34ad-4591-9f08-b95c548c7148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import csv\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a605d-0121-4e5c-b41a-92822e3ee01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ddc177-a209-42ac-8c82-32863b999eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def symbol_wize(y_true, y_pred):\n",
    "    y_true1, y_pred1 = y_true.split(), y_pred.split()\n",
    "    y_true, y_pred = set(), set()\n",
    "    \n",
    "    eps=1e-7\n",
    "    for i in y_true1:\n",
    "        y_true.update(set(range(int(i.split(':')[0]), int(i.split(':')[1]))))\n",
    "    for i in y_pred1:\n",
    "        y_pred.update(set(range(int(i.split(':')[0]), int(i.split(':')[1]))))\n",
    "    \n",
    "    true_pos = y_true.intersection(y_pred)\n",
    "    false_neg = y_true.difference(y_pred)\n",
    "    false_pos = y_pred.difference(y_true)\n",
    "    \n",
    "    precision = (len(true_pos)+eps)/(len(true_pos) + len(false_pos)+eps)\n",
    "    recall = (len(true_pos)+eps)/(len(true_pos) + len(false_neg)+eps)\n",
    "    \n",
    "    f1_score = 2*(precision*recall)/(precision + recall + 1e-7)\n",
    "    return f1_score\n",
    "\n",
    "def get_rank(true_class, pred_class, true_span, pred_span):\n",
    "    if true_class == pred_class == 1:\n",
    "        if type(true_class) == str and type(pred_class) == str:\n",
    "            return symbol_wize(true_span, pred_span)\n",
    "        else:\n",
    "            return 0\n",
    "    elif true_class == pred_class == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def gapping_metrics(true_data, pred_data, only_class=False):\n",
    "    f1_class = f1_score(true_data['class'].values, pred_data['class'].values)\n",
    "    \n",
    "    f1_symbolwise_score = 0\n",
    "    if not only_class:\n",
    "        f1_symbolwise_scores = []\n",
    "        for tag in ['cV', 'cR1', 'cR2', 'R1', 'R2']:\n",
    "            f1_symbolwise_scores += [get_rank(true_data.iloc[i]['class'], pred_data.iloc[i]['class'], true_data.iloc[i][tag], pred_data.iloc[i][tag]) for i in range(len(true_data))]\n",
    "        f1_symbolwise_score = np.mean(f1_symbolwise_scores)\n",
    "    \n",
    "    return {'f1_score': f1_class, 'f1_symbolwise_score': f1_symbolwise_score}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2e5576-7764-4c2f-a62b-584c9010d51c",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b90bf66-4b89-40de-8b0a-8b574bcc52a7",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e273099-0458-4a7f-9a35-751a8bfeb174",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join('data', 'train', 'train.csv'), sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "train_dict = train_df.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22258e1f-ad13-4230-906e-9306bf92bdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(os.path.join('data', 'test', 'test_gold_standard.csv'), sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "test_dict = test_df.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1a1a0a-a8ff-4116-8ae6-7ba6ba4cb976",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_MODEL = 'sberbank-ai/ruRoberta-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be54eca7-1e3b-4801-9f80-56bf761dc4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(NAME_MODEL,\n",
    "                                                    truncation=True,\n",
    "                                                    padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd915a9-72dd-4ca9-9ee1-5ff81202dfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAGS = [\"[NONE]\", \"cV\", \"cR1\", \"cR2\", \"R1\", \"R2\"]\n",
    "TAG2ID = {v: k for k, v in enumerate(TAGS)}\n",
    "\n",
    "GAPS = [\"[NONE]\", \"V\"]\n",
    "GAP2ID = {v: k for k, v in enumerate(GAPS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fe3f31-c990-41f0-8c0d-5619d98fd7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_text_data(train_dict):\n",
    "    text_data = []\n",
    "\n",
    "    for sample in tqdm(train_dict):\n",
    "        text = sample['text']\n",
    "        text = text.replace(\"—\", \"-\")\n",
    "\n",
    "        tokenizer_out = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "        word_ids = tokenizer_out.word_ids()\n",
    "        tokens_ids = tokenizer_out.input_ids\n",
    "        tokens = tokenizer_out.tokens()\n",
    "        #print(tokens)\n",
    "\n",
    "        tokens_borders = []\n",
    "        for i in range(0, len(tokens_ids)):\n",
    "            #print(tokens[i], tokens_ids[i])\n",
    "            if tokens_ids[i] == 0 or tokens[i] == '[SEP]' or tokens[i] == '[CLS]' or tokens[i] == '<s>' or tokens[i] == '</s>':\n",
    "                tokens_borders.append([-1, -1])\n",
    "            else:\n",
    "                token_border = tokenizer_out.token_to_chars(i)\n",
    "                tokens_borders.append([token_border.start, token_border.end])\n",
    "\n",
    "        tags_borders = []\n",
    "        for tag in TAGS[1:]:\n",
    "            if not pd.isna(sample[tag]):\n",
    "                for border in sample[tag].split(\" \"):\n",
    "                    left, right = list(map(int, border.split(\":\")))\n",
    "                    tags_borders.append((tag, left, right))\n",
    "\n",
    "        tags = []\n",
    "        for token_left, token_right in tokens_borders:\n",
    "\n",
    "            if token_left == -1 and token_right == -1:\n",
    "                tags.append(TAGS[0])\n",
    "                continue\n",
    "\n",
    "            flag = False\n",
    "            for tag, tag_left, tag_right in tags_borders:\n",
    "                if tag_left <= token_left and token_right <= tag_right:\n",
    "                    tags.append(tag)\n",
    "                    flag = True\n",
    "\n",
    "            if not flag:\n",
    "                tags.append(TAGS[0])   \n",
    "\n",
    "        gap_index = []\n",
    "        if not pd.isna(sample[\"V\"]):\n",
    "            for borders in sample[\"V\"].split(\" \"):\n",
    "                left, right = list(map(int, borders.split(\":\")))\n",
    "                gap_index.append(left)\n",
    "\n",
    "        gaps = [TAGS[0]] * len(tokens_ids)\n",
    "        for tag_left in gap_index:\n",
    "            flag = False\n",
    "            for i, (left, right) in enumerate(tokens_borders):\n",
    "                if tag_left == left and not flag:\n",
    "                    gaps[i] = \"V\"\n",
    "                    flag = True\n",
    "\n",
    "        #tags = [TAGS[0]] + tags + [TAGS[0]]\n",
    "        #gaps = [TAGS[0]] + gaps + [TAGS[0]]\n",
    "        #tokens_borders = [[-1, -1]] + tokens_borders + [[-1, -1]]\n",
    "        tags_ids = [TAG2ID[tag] for tag in tags]\n",
    "        gaps_ids = [GAP2ID[gap] for gap in gaps]\n",
    "        attention_mask = tokenizer_out['attention_mask']\n",
    "        label = int(sample['class'])\n",
    "\n",
    "        text_data.append({\n",
    "            'tokens_ids': tokens_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'tags_ids': tags_ids,\n",
    "            'gaps_ids': gaps_ids,\n",
    "            'label': label,\n",
    "            'tokens_borders': tokens_borders\n",
    "        })\n",
    "    \n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2385262a-1fb7-47b4-863c-88aa00ac7fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARRGDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text_data):\n",
    "        self.text_data = text_data\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val, dtype=torch.long) for key, val in self.text_data[idx].items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5c0e88-1ac3-4f63-9b4e-c689d223195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = make_text_data(train_dict)\n",
    "arrg_dataset_train = ARRGDataset(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8f3c59-0400-433b-b175-cb85d7d286d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = make_text_data(test_dict)\n",
    "arrg_dataset_test = ARRGDataset(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39bce47-15d5-4ae9-8f01-6077645fb47b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c5a330-0a1b-45a5-8944-27e66f2100f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertModel, BertConfig, AutoConfig, AutoModel, DistilBertConfig\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, BertForSequenceClassification\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForTokenClassification, DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6e0296-50cb-43b6-92d5-7540cf4168b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BertAgrrModel(transformers.PreTrainedModel):\n",
    "\n",
    "    def __init__(self, name='DeepPavlov/rubert-base-cased-sentence', out_size=768):\n",
    "        super(BertAgrrModel, self).__init__(config=AutoConfig.from_pretrained(name, output_last_hidden_state=True))\n",
    "        #config = BertConfig.from_pretrained(\"distilbert-base-uncased\", output_last_hidden_state=True)\n",
    "        self.bert = AutoModel.from_pretrained(name)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.sentence_classifier = nn.Linear(out_size, 2)\n",
    "        self.full_annotation_classifier = nn.Linear(out_size, 6)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids, attention_mask)\n",
    "        \n",
    "        sequence_output = output.last_hidden_state\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        pooled_output = sequence_output[:, 0, :]\n",
    "        \n",
    "        sentence_logits = self.sentence_classifier(pooled_output)\n",
    "        full_annotation_logits = self.full_annotation_classifier(sequence_output)\n",
    "        sentence_probs = torch.softmax(sentence_logits, dim=1)\n",
    "        full_annotation_probs = torch.softmax(full_annotation_logits, dim=2)\n",
    "        \n",
    "        return {\n",
    "            'sentence_logits': sentence_logits,\n",
    "            'full_annotation_logits': full_annotation_logits,\n",
    "            'sentence_probs': sentence_probs,\n",
    "            'full_annotation_probs': full_annotation_probs\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5821a2ea-9319-402e-bcbe-de7361ba9866",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b241689b-29fc-46d3-b9d4-c3821f7f8bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886bc98a-5a98-4b73-a7d9-191072a56738",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 15\n",
    "device = 'cuda:4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009fddae-0190-44b3-bd64-1ed670983dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertAgrrModel(NAME_MODEL).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b4bde7-d9bd-4b42-86c1-020c46274745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "train_dataloader = DataLoader(arrg_dataset_train, shuffle=True, batch_size=24)\n",
    "test_dataloader = DataLoader(arrg_dataset_test, shuffle=False, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaf45ba-e42c-4b09-b97a-95735b542b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bf766f-357b-4d2d-b86f-00be8a8c7160",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = []\n",
    "all_loss = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCH):\n",
    "    num_batches = 0\n",
    "    losses = []\n",
    "    \n",
    "    model.eval()\n",
    "    max_score = -1.0\n",
    "    print('EPOCH: {} Starting eval...'.format(epoch+1))\n",
    "    pred_labeles = []\n",
    "    pred_cV = []\n",
    "    pred_cR1 = []\n",
    "    pred_cR2 = []\n",
    "    pred_R1 = []\n",
    "    pred_R2 = []\n",
    "    \n",
    "    logits = []\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        token_ids = batch['tokens_ids']\n",
    "        mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "        tags = batch['tags_ids']\n",
    "        tokens_borders = batch['tokens_borders'].detach().numpy()\n",
    "        \n",
    "        out = model(token_ids.to(device), mask.to(device))\n",
    "        \n",
    "        class_probs = out['sentence_probs'].detach().to('cpu').numpy()[:, 1]\n",
    "        logits += list(class_probs)\n",
    "        class_probs[class_probs >= 0.5] = 1\n",
    "        class_probs[class_probs < 0.5] = 0\n",
    "        pred_labeles += list(class_probs)\n",
    "        \n",
    "        full_annotation_probs = out['full_annotation_probs'].detach().to('cpu').numpy()\n",
    "        full_annotation_class = np.argmax(full_annotation_probs, axis=2)\n",
    "        \n",
    "        pred_tags = {\n",
    "            'cV': '0:0',\n",
    "            'cR1': '0:0',\n",
    "            'cR2': '0:0',\n",
    "            'R1': '0:0',\n",
    "            'R2': '0:0'\n",
    "        }\n",
    "        for border, annot in zip(tokens_borders, full_annotation_class):\n",
    "            for tag_id in [1, 2, 3, 4, 5]:\n",
    "                border_tag = border[annot == tag_id]\n",
    "                \n",
    "                if border_tag.shape[0] != 0:\n",
    "                    left = 0\n",
    "                    right = 0\n",
    "                    for b in border_tag:\n",
    "                        l, r = b[0], b[1]\n",
    "                        \n",
    "                        if l == -1 and r == -1:\n",
    "                            continue\n",
    "                        \n",
    "                        if l <= left:\n",
    "                            left = l\n",
    "                        if r >= right:\n",
    "                            right = r\n",
    "                \n",
    "                pred_tags[TAGS[tag_id]] = str(left) + ':' + str(right)\n",
    "        \n",
    "            pred_cV.append(pred_tags['cV'])\n",
    "            pred_cR1.append(pred_tags['cR1'])\n",
    "            pred_cR2.append(pred_tags['cR2'])\n",
    "            pred_R1.append(pred_tags['R1'])\n",
    "            pred_R2.append(pred_tags['R2'])\n",
    "    \n",
    "    pred_df = pd.DataFrame({\n",
    "        'class': pred_labeles,\n",
    "        'cV': pred_cV,\n",
    "        'cR1': pred_cR1,\n",
    "        'cR2': pred_cR2,\n",
    "        'R1': pred_R1,\n",
    "        'R2': pred_R2\n",
    "    })\n",
    "\n",
    "    metrics = gapping_metrics(test_df, pred_df)\n",
    "    \n",
    "    if metrics['f1_score'] > max_score:\n",
    "        print('Saving model...')\n",
    "        max_score = metrics['f1_score']\n",
    "        torch.save(model.state_dict(), os.path.join('checkpoints', NAME_MODEL + '_' + str(max_score) + '.model'))\n",
    "    \n",
    "    print('Eval metrics:', metrics)\n",
    "    f1_scores.append(metrics)\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        token_ids = batch['tokens_ids']\n",
    "        mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "        tags = batch['tags_ids']\n",
    "        \n",
    "        labels = labels.type(torch.long).to(device)\n",
    "        out = model(token_ids.to(device), mask.to(device))\n",
    "        \n",
    "        sentence_loss = criterion(out['sentence_probs'], labels)\n",
    "        \n",
    "        loss = sentence_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if (i+1) % 64 == 0:\n",
    "            print('Step: {}, Loss: {}'.format(i+1, np.mean(losses)))\n",
    "            all_loss.append(np.mean(losses))\n",
    "            losses = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580c0090-79d4-497f-9b15-dfcbb7b8d2ed",
   "metadata": {},
   "source": [
    "# UD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2abf248-0d76-47bc-a3b8-7e372e95e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from deeppavlov import build_model, configs\n",
    "from deeppavlov.models.morpho_tagger.common import call_model\n",
    "\n",
    "from ufal.udpipe import Model as udModel, Pipeline\n",
    "\n",
    "from Gapping.read_write import read_data, parse_ud_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9906fa7-99d5-404c-9bc3-a04cdbc88941",
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_model_path = \"models/russian-syntagrus-ud-2.3-181115.udpipe\"\n",
    "train_path = \"Gapping/data/train.csv\"\n",
    "outfile = \"results/train.out\"\n",
    "tokenize, parse = False, True\n",
    "tokenized_outfile = None\n",
    "\n",
    "\n",
    "HYPHENS = \"-—–\"\n",
    "QUOTES = \"«“”„»``''\"\n",
    "\n",
    "def cannot_be_before_hyphen(x):\n",
    "    return not (x.isalpha() or x.isdigit() or x in HYPHENS)       \n",
    "\n",
    "def fix_quotes(x):\n",
    "    answer = []\n",
    "    lines = x.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line == \"\":\n",
    "            continue\n",
    "        splitted = line.split(\"\\t\")\n",
    "        if splitted[1] in QUOTES:\n",
    "            splitted[1] = splitted[2] = '\"'\n",
    "            splitted[3] = \"PUNCT\"\n",
    "            splitted[5] = \"_\"\n",
    "        answer.append(\"\\t\".join(splitted))\n",
    "    answer = \"\\n\".join(answer)\n",
    "    return answer\n",
    "    \n",
    "def sanitize(sent):\n",
    "    sent = \"\".join(a if a not in QUOTES else '\"' for a in sent)\n",
    "    answer = \"\"\n",
    "    indexes = [0] + [i for i, a in enumerate(sent) if a in HYPHENS] + [len(sent)]\n",
    "    start = 0\n",
    "    for i, hyphen_index in enumerate(indexes[1:-1], 1):\n",
    "        answer += sent[start:hyphen_index]\n",
    "        if hyphen_index > 0 and hyphen_index < len(sent) - 1 and cannot_be_before_hyphen(sent[hyphen_index+1]) and sent[hyphen_index-1].isalpha():\n",
    "            answer += \" \" + sent[hyphen_index]\n",
    "            if sent[hyphen_index+1] != \" \":\n",
    "                answer += \" \"\n",
    "        elif hyphen_index > 0 and hyphen_index < len(sent) - 1 and cannot_be_before_hyphen(sent[hyphen_index-1]) and sent[hyphen_index+1].isalpha():\n",
    "            if sent[hyphen_index-1] != \" \":\n",
    "                answer += \" \"\n",
    "            answer += sent[hyphen_index] + \" \"\n",
    "        else:\n",
    "            answer += sent[hyphen_index]\n",
    "        start = hyphen_index + 1\n",
    "    answer += sent[start:]\n",
    "    return answer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = build_model(configs.morpho_tagger.UD2_0.morpho_ru_syntagrus_pymorphy_lemmatize, download=True)\n",
    "    ud_model = udModel.load(ud_model_path)\n",
    "    sents, answers = read_data(train_path)\n",
    "    symbols = sorted(set(a for sent in sents for a in sent))\n",
    "    sents = [sanitize(sent) for sent in sents]\n",
    "    \n",
    "    if tokenize:\n",
    "        tokenized_data, for_tagging = \"\", []\n",
    "        tokenizer = Pipeline(ud_model, \"tokenize\", Pipeline.NONE, Pipeline.NONE, \"conllu\")\n",
    "        for start in range(0, len(sents), 40):\n",
    "            if start % 400 == 0:\n",
    "                print(\"{} sents processed\".format(start))\n",
    "            end = min(start + 40, len(sents))\n",
    "            curr_output = tokenizer.process(\"\\n\\n\".join(sents[start:end]))\n",
    "            tokenized_data += curr_output + \"\\n\"\n",
    "            curr_output = parse_ud_output(curr_output)\n",
    "            for_tagging.extend([[elem[1] for elem in sent] for sent in curr_output])\n",
    "        if tokenized_outfile is not None:\n",
    "            with open(tokenized_outfile, \"w\", encoding=\"utf8\") as fout:\n",
    "                fout.write(tokenized_data)\n",
    "    else:\n",
    "        for_tagging = sents\n",
    "    if parse:\n",
    "        print(len(for_tagging))\n",
    "        print(\"Tagging...\")\n",
    "        tagged_data = call_model(model, for_tagging, batch_size=64)\n",
    "        tagged_data = [fix_quotes(elem) for elem in tagged_data]\n",
    "        print(\"Tagging completed...\")\n",
    "        parser = Pipeline(ud_model, \"conllu\", Pipeline.NONE, Pipeline.DEFAULT, \"conllu\")\n",
    "        with open(outfile, \"w\", encoding=\"utf8\") as fout:\n",
    "            for start in range(0, len(tagged_data), 16):\n",
    "                if start % 400 == 0:\n",
    "                    print(\"{} sents processed\".format(start))\n",
    "                end = min(start + 16, len(tagged_data))\n",
    "                parsed_data = parser.process(\"\\n\\n\".join(tagged_data[start:end]))\n",
    "                fout.write(parsed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3ca078-aa23-4429-89bc-5554422ed68d",
   "metadata": {},
   "source": [
    "# GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff63f84-d5bb-4290-8a82-a8bb729fcdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ['TORCH_VERSION'] = torch.__version__\n",
    "!echo $TORCH_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3ed293-e512-4dce-98ed-78f17808effe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH_VERSION}.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH_VERSION}.html\n",
    "!pip install torch-cluster -f https://data.pyg.org/whl/torch-${TORCH_VERSION}.html\n",
    "!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-${TORCH_VERSION}.html\n",
    "!pip install torch-geometric -f https://data.pyg.org/whl/torch-${TORCH_VERSION}.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00718dd9-7d97-4f5b-8e9a-ec0881711737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "import youtokentome as yttm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9114f1fb-648e-4a0a-87df-c6fd4032fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UDDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(UDDataset, self).__init__(root, transform, pre_transform)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['train.out', 'train.csv']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return 'no.pt'\n",
    "    \n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        self.PROP = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM',\n",
    "                     'PAD', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
    "        \n",
    "        self.parsed_sen = []\n",
    "        with open(self.raw_paths[0], 'r', encoding=\"utf8\") as parsed_file:\n",
    "            one_parsed_sen = []\n",
    "            for i, line in enumerate(parsed_file):\n",
    "                line = line.strip()\n",
    "\n",
    "                if line == '':\n",
    "                    self.parsed_sen.append(one_parsed_sen)\n",
    "                    one_parsed_sen = []\n",
    "                    continue\n",
    "                \n",
    "                one_parsed_sen.append(line.split('\\t'))\n",
    "        \n",
    "        \n",
    "        idx = 0\n",
    "        for ud_graph, label in tqdm(zip(self.parsed_sen,\n",
    "                                        list(pd.read_csv(self.raw_paths[1], sep='\\t', encoding='utf-8')['class'].values))):\n",
    "            edge_index = []\n",
    "            x = []\n",
    "            for term in ud_graph:\n",
    "                if int(term[-4]) == 0:\n",
    "                    feat_prop = [0] * len(self.PROP)\n",
    "                    feat_prop[self.PROP.index(term[3])] = 1\n",
    "                    x.append(feat_prop)\n",
    "                    continue\n",
    "\n",
    "                feat_prop = [0] * len(self.PROP)\n",
    "                feat_prop[self.PROP.index(term[3])] = 1\n",
    "                x.append(feat_prop)\n",
    "\n",
    "                edge_index.append([int(term[0])-1, int(term[-4])-1])\n",
    "\n",
    "            x = torch.tensor(np.array(x))\n",
    "\n",
    "            edge_index = np.array(edge_index)\n",
    "            edge_index = edge_index.T\n",
    "            edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "            #print(edge_index)\n",
    "            \n",
    "            label = torch.tensor([label])\n",
    "            torch.save(Data(x=x, edge_index=edge_index, y=label),\n",
    "                       os.path.join(self.processed_dir, f'UD_{idx}.pt'))\n",
    "            idx += 1\n",
    "        \n",
    "    def len(self):\n",
    "        return len(self.parsed_sen)\n",
    "    \n",
    "    def get(self, idx):\n",
    "        return torch.load(os.path.join(self.processed_dir, f'UD_{idx}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853b021f-d63c-4960-8bae-6c3227d45339",
   "metadata": {},
   "outputs": [],
   "source": [
    "ud_dataset = UDDataset('/content/drive/MyDrive/Диплом/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d620bb01-7a7e-4d75-ab3e-651c09417843",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "loader = DataLoader(ud_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af00fd-7ec0-45be-af04-4b45b3875099",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(ud_dataset[:int(len(ud_dataset) * 0.8)], batch_size=batch_size)\n",
    "val_loader = DataLoader(ud_dataset[int(len(ud_dataset) * 0.8):int(len(ud_dataset) * 0.9)], batch_size=batch_size)\n",
    "test_loader = DataLoader(ud_dataset[int(len(ud_dataset) * 0.9):], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd4b2b0-cc58-439f-8dd4-c6a8bd471630",
   "metadata": {},
   "source": [
    "# GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c266d174-68ff-4650-92df-d549d6195885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6de08a-3b8e-4cb9-b6d1-7cfd0db58948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class GCN(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCN, self).__init__(aggr='add')\n",
    "        self.lin = nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        x = self.lin(x)\n",
    "\n",
    "        norm = 1 / torch.sqrt(\n",
    "            degree(edge_index[0])[edge_index[0]] * \n",
    "            degree(edge_index[1])[edge_index[1]]\n",
    "        )\n",
    "\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02215892-83c5-4882-b7a7-cda2ede1d28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.0):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        self.convs = pyg_nn.Sequential(\n",
    "            'x, edge_index', [\n",
    "                (GCN(input_dim, hidden_dim), 'x, edge_index -> x'),\n",
    "                nn.ReLU(),\n",
    "                (GCN(hidden_dim, hidden_dim), 'x, edge_index -> x'),\n",
    "                nn.ReLU(),\n",
    "                (GCN(hidden_dim, hidden_dim), 'x, edge_index -> x'),\n",
    "                nn.ReLU(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.post_mp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x.type(torch.FloatTensor), data.edge_index, data.batch\n",
    "        \n",
    "        x = self.convs(x, edge_index)\n",
    "        x = pyg_nn.global_max_pool(x, batch)\n",
    "        x = self.post_mp(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f942352-ac7d-4cde-9683-3a103ab39101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(x, labels):\n",
    "    return F.cross_entropy(x, labels)\n",
    "\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, epochs):\n",
    "    train_loss = []\n",
    "    val_accuracy = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        batch_train_loss = []\n",
    "        batch_val_accuracy = []\n",
    "\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            logits = model(batch)\n",
    "            labels = batch.y\n",
    "            loss = cross_entropy_loss(logits, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch_train_loss.append(float(loss.data.numpy()))\n",
    "\n",
    "        train_loss.append(np.mean(batch_train_loss))\n",
    "\n",
    "        model.eval()\n",
    "        for batch in val_loader:\n",
    "            pred = torch.argmax(model(batch), dim=1)\n",
    "            labels = batch.y\n",
    "            batch_val_accuracy.append(np.mean((labels == pred).numpy()))\n",
    "\n",
    "        val_accuracy.append(np.mean(batch_val_accuracy))\n",
    "        \n",
    "    return model, train_loss, val_accuracy\n",
    "\n",
    "\n",
    "def plot_progress(train_loss, val_accuracy):\n",
    "    fig, (train_ax, val_ax) = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "    train_ax.plot(train_loss)\n",
    "    train_ax.set_title('Train loss')\n",
    "    train_ax.set_xlabel('Epoch')\n",
    "\n",
    "    val_ax.plot(val_accuracy)\n",
    "    val_ax.set_title('Val F1')\n",
    "    val_ax.set_xlabel('Epoch')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "\n",
    "    predictions = np.array([])\n",
    "    labels = np.array([])\n",
    "\n",
    "    for batch in loader:\n",
    "        pred = torch.argmax(model(batch), dim=1)\n",
    "        true = batch.y\n",
    "\n",
    "        predictions = np.append(predictions, pred)\n",
    "        labels = np.append(labels, true)\n",
    "\n",
    "    return np.mean(predictions == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21644687-b35b-45a4-ab46-1fd40b4a4bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "hidden_dim = 5\n",
    "model = GNN(18, hidden_dim, 2, 0.2)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d926971-0fba-41a4-9ab1-a30e6cc530f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "model, train_loss, val_accuracy = train(model, optimizer, train_loader, val_loader, epochs)\n",
    "plot_progress(train_loss, val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bc9b90-d14b-48cc-b944-3c33d98a493b",
   "metadata": {},
   "source": [
    "# GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abec5b25-4d7e-4c45-8c9e-6a243e2c3cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "class GAT(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, num_heads):\n",
    "        super(GAT, self).__init__(aggr='add')\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.lin = nn.Linear(self.in_channels, self.num_heads * self.out_channels, bias=False)\n",
    "        self.att = nn.Parameter(torch.Tensor(1, self.num_heads, 2 * out_channels))\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.att)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return self.propagate(edge_index=edge_index, x=x)\n",
    "\n",
    "    def message(self, x_i, x_j, index):\n",
    "        x_i = x_i.view(-1, self.num_heads, self.out_channels)\n",
    "        x_j = x_j.view(-1, self.num_heads, self.out_channels)\n",
    "        \n",
    "        concatenated_features = torch.cat([x_i, x_j], dim=-1)\n",
    "\n",
    "        alpha = (concatenated_features * self.att).sum(dim=-1).unsqueeze(-1)\n",
    "        alpha = F.leaky_relu(alpha)\n",
    "        alpha = pyg_utils.softmax(alpha, index=index)\n",
    "        \n",
    "        return (alpha * x_j).mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fe8768-5db7-4a0a-bc7c-5e0e330d92ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_heads, dropout=0.0):\n",
    "        super(GNN, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.convs = self.convs = pyg_nn.Sequential(\n",
    "            'x, edge_index', [\n",
    "                (GAT(input_dim, hidden_dim, num_heads), 'x, edge_index -> x'),\n",
    "                nn.ReLU(),\n",
    "                (GAT(hidden_dim, hidden_dim, num_heads), 'x, edge_index -> x'),\n",
    "                nn.ReLU(),\n",
    "                (GAT(hidden_dim, hidden_dim, num_heads), 'x, edge_index -> x'),\n",
    "                nn.ReLU(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.post_mp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        x = self.convs(x, edge_index)\n",
    "        x = pyg_nn.global_max_pool(x, batch)\n",
    "        \n",
    "        x = self.post_mp(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10600d6-13f3-4c58-b693-0dc6b01783d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "hidden_dim = 5\n",
    "model = GNN(input_dim=18, hidden_dim, output_dim=2, num_heads=3, 0.2)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70de2331-c224-4f82-a678-7677c70b4ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "model, train_loss, val_accuracy = train(model, optimizer, train_loader, val_loader, epochs)\n",
    "plot_progress(train_loss, val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84138ba3-32f1-48d7-8bce-b841aa719fbb",
   "metadata": {},
   "source": [
    "# Full annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41182c59-62ba-4a5e-9889-56ba9aad439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_heads=1, dropout=0.0):\n",
    "        super(GNN, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GCN(input_dim, hidden_dim))\n",
    "        for l in range(self.num_layers - 1):\n",
    "            self.convs.append(GCN(hidden_dim, hidden_dim))\n",
    "\n",
    "        self.post_mp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(self.dropout), \n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "        \n",
    "        x = self.post_mp(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784be8cb-07b8-4688-a980-b794c11f5c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_node_classification(model, optimizer, graph, epochs):\n",
    "    train_loss = []\n",
    "    val_accuracy = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "\n",
    "        logits = model(graph)\n",
    "        \n",
    "        train_logits = logits[graph.train_mask]\n",
    "        train_labels = graph.y[graph.train_mask]\n",
    "\n",
    "        loss = cross_entropy_loss(train_logits, train_labels)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss.append(loss.detach().numpy())\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        pred = model(graph).max(dim=1)[1]\n",
    "        \n",
    "        val_pred = pred[graph.val_mask]\n",
    "        val_labels = graph.y[graph.val_mask]\n",
    "        \n",
    "        val_accuracy.append(np.mean((val_labels == val_pred).numpy()))\n",
    "        \n",
    "    return model, train_loss, val_accuracy\n",
    "\n",
    "\n",
    "def evaluate_node_classification(model, graph):\n",
    "    model.eval()\n",
    "    mask = graph.test_mask\n",
    "\n",
    "    predictions = model(graph).max(dim=1)[1].numpy()[graph.test_mask]\n",
    "    labels = graph.y.numpy()[graph.test_mask]\n",
    "\n",
    "    return np.mean(predictions == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8ea296-4238-4b96-967c-0962184c77e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 3\n",
    "dropout = 0.2\n",
    "hidden_dim = 16\n",
    "lr = 0.001\n",
    "\n",
    "model = GNN(dataset.num_node_features, hidden_dim, dataset.num_classes, num_layers, dropout)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab35310-56ec-4429-be0c-f207825372e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "model, train_loss, val_accuracy = train_for_node_classification(model, optimizer, dataset[0], epochs)\n",
    "plot_progress(train_loss, val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dff19c-e6dd-4fb4-bd5b-bfbc8f4da81f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df14cd21-d5ca-44bb-9312-42c3577a3738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
