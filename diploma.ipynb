{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90aa36a1-7e91-4c4b-a139-221cf1ade998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import csv\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "235aefbc-8992-417a-9a6e-aeaed26625f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09db9a24-c98f-4a6f-a615-09f2a82bf265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def symbol_wize(y_true, y_pred):\n",
    "    y_true1, y_pred1 = y_true.split(), y_pred.split()\n",
    "    y_true, y_pred = set(), set()\n",
    "    \n",
    "    eps=1e-7\n",
    "    for i in y_true1:\n",
    "        y_true.update(set(range(int(i.split(':')[0]), int(i.split(':')[1]))))\n",
    "    for i in y_pred1:\n",
    "        y_pred.update(set(range(int(i.split(':')[0]), int(i.split(':')[1]))))\n",
    "    \n",
    "    true_pos = y_true.intersection(y_pred)\n",
    "    false_neg = y_true.difference(y_pred)\n",
    "    false_pos = y_pred.difference(y_true)\n",
    "    \n",
    "    precision = (len(true_pos)+eps)/(len(true_pos) + len(false_pos)+eps)\n",
    "    recall = (len(true_pos)+eps)/(len(true_pos) + len(false_neg)+eps)\n",
    "    \n",
    "    f1_score = 2*(precision*recall)/(precision + recall + 1e-7)\n",
    "    return f1_score\n",
    "\n",
    "def get_rank(true_class, pred_class, true_span, pred_span):\n",
    "    if true_class == pred_class == 1:\n",
    "        return symbol_wize(true_span, pred_span)\n",
    "    elif true_class == pred_class == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def gapping_metrics(true_data, pred_data, only_class=False):\n",
    "    f1_class = f1_score(true_data['class'].values, pred_data['class'].values)\n",
    "    \n",
    "    f1_symbolwise_score = 0\n",
    "    if not only_class:\n",
    "        f1_symbolwise_scores = []\n",
    "        for tag in ['cV', 'V', 'cR1', 'cR2', 'R1', 'R2']:\n",
    "            f1_symbolwise_scores += [get_rank(true_data.iloc[i]['class'], pred_data.iloc[i]['class'], true_data.iloc[i][tag], pred_data.iloc[i][tag]) for i in range(len(true_data))]\n",
    "        f1_symbolwise_score = np.mean(f1_symbolwise_scores)\n",
    "    \n",
    "    return {'f1_score': f1_class, 'f1_symbolwise_score': f1_symbolwise_score}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b51ad5-91c2-494f-a957-a8ab9004d1bf",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0af2df2-3df2-45c1-a1b7-a53ed45473ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join('data', 'train', 'train.csv'), sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "train_dict = train_df.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f59ed7c-f0ba-4657-ae68-4ee7da2ac952",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(os.path.join('data', 'test', 'test_gold_standard.csv'), sep=\"\\t\", quoting=csv.QUOTE_NONE)\n",
    "test_dict = test_df.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74d818b0-be7f-42ab-aba1-330482f5182b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>cV</th>\n",
       "      <th>cR1</th>\n",
       "      <th>cR2</th>\n",
       "      <th>V</th>\n",
       "      <th>R1</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Изобретение относится к судостроению и касаетс...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Эти состояния называют фазами воды, а превраще...</td>\n",
       "      <td>1</td>\n",
       "      <td>14:22</td>\n",
       "      <td>0:13</td>\n",
       "      <td>23:34</td>\n",
       "      <td>81:81</td>\n",
       "      <td>38:78</td>\n",
       "      <td>81:100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>И должен ни единой долькой  Не отступаться от ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Он потребовал обеспечить полное осуществление ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>По мнению местного пастора Элла Эбанкса, запре...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040</th>\n",
       "      <td>Кто-то ходит в кино с девушкой, а кто-то со ст...</td>\n",
       "      <td>1</td>\n",
       "      <td>7:12</td>\n",
       "      <td>0:6</td>\n",
       "      <td>20:30</td>\n",
       "      <td>41:41</td>\n",
       "      <td>34:40</td>\n",
       "      <td>41:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2041</th>\n",
       "      <td>Восстановление показателей банка будет идти бо...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2042</th>\n",
       "      <td>Вы кое-что смыслите, в лабораторной технике; в...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043</th>\n",
       "      <td>Если готовка не ваш конек, то вас выручить наш...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044</th>\n",
       "      <td>И я жила в вечном страхе, что он найдет меня р...</td>\n",
       "      <td>1</td>\n",
       "      <td>33:39</td>\n",
       "      <td>30:32</td>\n",
       "      <td>40:44</td>\n",
       "      <td>59:59</td>\n",
       "      <td>57:58</td>\n",
       "      <td>59:62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2045 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  class     cV    cR1  \\\n",
       "0     Изобретение относится к судостроению и касаетс...      0    NaN    NaN   \n",
       "1     Эти состояния называют фазами воды, а превраще...      1  14:22   0:13   \n",
       "2     И должен ни единой долькой  Не отступаться от ...      0    NaN    NaN   \n",
       "3     Он потребовал обеспечить полное осуществление ...      0    NaN    NaN   \n",
       "4     По мнению местного пастора Элла Эбанкса, запре...      0    NaN    NaN   \n",
       "...                                                 ...    ...    ...    ...   \n",
       "2040  Кто-то ходит в кино с девушкой, а кто-то со ст...      1   7:12    0:6   \n",
       "2041  Восстановление показателей банка будет идти бо...      0    NaN    NaN   \n",
       "2042  Вы кое-что смыслите, в лабораторной технике; в...      0    NaN    NaN   \n",
       "2043  Если готовка не ваш конек, то вас выручить наш...      0    NaN    NaN   \n",
       "2044  И я жила в вечном страхе, что он найдет меня р...      1  33:39  30:32   \n",
       "\n",
       "        cR2      V     R1      R2  \n",
       "0       NaN    NaN    NaN     NaN  \n",
       "1     23:34  81:81  38:78  81:100  \n",
       "2       NaN    NaN    NaN     NaN  \n",
       "3       NaN    NaN    NaN     NaN  \n",
       "4       NaN    NaN    NaN     NaN  \n",
       "...     ...    ...    ...     ...  \n",
       "2040  20:30  41:41  34:40   41:53  \n",
       "2041    NaN    NaN    NaN     NaN  \n",
       "2042    NaN    NaN    NaN     NaN  \n",
       "2043    NaN    NaN    NaN     NaN  \n",
       "2044  40:44  59:59  57:58   59:62  \n",
       "\n",
       "[2045 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "56a0e4b7-054f-4c3c-aee7-800136bd4a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_MODEL = 'sberbank-ai/ruRoberta-large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b971946e-4c59-444c-b0aa-dc3cc0bfea9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ee21ec0459405e9b70fe9e0b7e0076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/674 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e66791c198e4b409243e94a47db69f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.73M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c141e41bc044018a779979aadc9f328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(NAME_MODEL,\n",
    "                                                    truncation=True,\n",
    "                                                    padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d5800c3d-6617-4fd3-bfad-8c8d6d4d9498",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAGS = [\"[NONE]\", \"cV\", \"cR1\", \"cR2\", \"R1\", \"R2\"]\n",
    "TAG2ID = {v: k for k, v in enumerate(TAGS)}\n",
    "\n",
    "GAPS = [\"[NONE]\", \"V\"]\n",
    "GAP2ID = {v: k for k, v in enumerate(GAPS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b1bf0763-e951-484d-86b8-ae5167e4f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_text_data(train_dict):\n",
    "    text_data = []\n",
    "\n",
    "    for sample in tqdm(train_dict):\n",
    "        text = sample['text']\n",
    "        text = text.replace(\"—\", \"-\")\n",
    "\n",
    "        tokenizer_out = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
    "        word_ids = tokenizer_out.word_ids()\n",
    "        tokens_ids = tokenizer_out.input_ids\n",
    "        tokens = tokenizer_out.tokens()\n",
    "        #print(tokens)\n",
    "\n",
    "        tokens_borders = []\n",
    "        for i in range(0, len(tokens_ids)):\n",
    "            #print(tokens[i], tokens_ids[i])\n",
    "            if tokens_ids[i] == 0 or tokens[i] == '[SEP]' or tokens[i] == '[CLS]' or tokens[i] == '<s>' or tokens[i] == '</s>':\n",
    "                tokens_borders.append([-1, -1])\n",
    "            else:\n",
    "                token_border = tokenizer_out.token_to_chars(i)\n",
    "                tokens_borders.append([token_border.start, token_border.end])\n",
    "\n",
    "        tags_borders = []\n",
    "        for tag in TAGS[1:]:\n",
    "            if not pd.isna(sample[tag]):\n",
    "                for border in sample[tag].split(\" \"):\n",
    "                    left, right = list(map(int, border.split(\":\")))\n",
    "                    tags_borders.append((tag, left, right))\n",
    "\n",
    "        tags = []\n",
    "        for token_left, token_right in tokens_borders:\n",
    "\n",
    "            if token_left == -1 and token_right == -1:\n",
    "                tags.append(TAGS[0])\n",
    "                continue\n",
    "\n",
    "            flag = False\n",
    "            for tag, tag_left, tag_right in tags_borders:\n",
    "                if tag_left <= token_left and token_right <= tag_right:\n",
    "                    tags.append(tag)\n",
    "                    flag = True\n",
    "\n",
    "            if not flag:\n",
    "                tags.append(TAGS[0])   \n",
    "\n",
    "        gap_index = []\n",
    "        if not pd.isna(sample[\"V\"]):\n",
    "            for borders in sample[\"V\"].split(\" \"):\n",
    "                left, right = list(map(int, borders.split(\":\")))\n",
    "                gap_index.append(left)\n",
    "\n",
    "        gaps = [TAGS[0]] * len(tokens_ids)\n",
    "        for tag_left in gap_index:\n",
    "            flag = False\n",
    "            for i, (left, right) in enumerate(tokens_borders):\n",
    "                if tag_left == left and not flag:\n",
    "                    gaps[i] = \"V\"\n",
    "                    flag = True\n",
    "\n",
    "        #tags = [TAGS[0]] + tags + [TAGS[0]]\n",
    "        #gaps = [TAGS[0]] + gaps + [TAGS[0]]\n",
    "        #tokens_borders = [[-1, -1]] + tokens_borders + [[-1, -1]]\n",
    "        tags_ids = [TAG2ID[tag] for tag in tags]\n",
    "        gaps_ids = [GAP2ID[gap] for gap in gaps]\n",
    "        attention_mask = tokenizer_out['attention_mask']\n",
    "        label = int(sample['class'])\n",
    "\n",
    "        text_data.append({\n",
    "            'tokens_ids': tokens_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'tags_ids': tags_ids,\n",
    "            'gaps_ids': gaps_ids,\n",
    "            'label': label,\n",
    "            'tokens_borders': tokens_borders\n",
    "        })\n",
    "    \n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "19c04d4b-6f83-4d62-be83-a7e051d91050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARRGDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text_data):\n",
    "        self.text_data = text_data\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val, dtype=torch.long) for key, val in self.text_data[idx].items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "226698b2-df1e-4459-bc2d-362916e10be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16406/16406 [00:11<00:00, 1483.17it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = make_text_data(train_dict)\n",
    "arrg_dataset_train = ARRGDataset(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "01cff022-a900-429e-9d91-02b372892f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2045/2045 [00:00<00:00, 2150.88it/s]\n"
     ]
    }
   ],
   "source": [
    "test_data = make_text_data(test_dict)\n",
    "arrg_dataset_test = ARRGDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a065831f-d061-4359-84ec-e2a958b5f80e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens_ids': tensor([   101,  19597,  33276,  40035,  16241,  34870,    551,  36681,  33580,\n",
       "          76688,  22325,    549,  56280,  12016,  14317,  32243,  46672,  90068,\n",
       "          32875,  91258,  53204,  63596,  10227,  94826,  86079,    117,  14028,\n",
       "            570,  18705,  11078,  16111,  16583,  84190,  10625,  10913,  30148,\n",
       "          23879,  35459,    543,  11613,  15755,  12861,  10122,  63596,  45803,\n",
       "          20265,    549, 105805,  83013,  30148,  57935,  19364,  10970,  10439,\n",
       "          10517,  10267,    549,    556,  29952,  83856,  10353,    552,  60545,\n",
       "          10433,    117,  58742,  36069,  10385,  13686,  53204,  10990,  32532,\n",
       "          35131,    119,    102,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'tags_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'gaps_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'label': tensor(0),\n",
       " 'tokens_borders': tensor([[ -1,  -1],\n",
       "         [  0,   2],\n",
       "         [  2,   4],\n",
       "         [  4,   7],\n",
       "         [  7,  11],\n",
       "         [ 12,  21],\n",
       "         [ 22,  23],\n",
       "         [ 24,  27],\n",
       "         [ 27,  30],\n",
       "         [ 30,  33],\n",
       "         [ 33,  36],\n",
       "         [ 37,  38],\n",
       "         [ 39,  41],\n",
       "         [ 41,  43],\n",
       "         [ 43,  47],\n",
       "         [ 48,  52],\n",
       "         [ 52,  54],\n",
       "         [ 54,  56],\n",
       "         [ 56,  63],\n",
       "         [ 64,  68],\n",
       "         [ 68,  70],\n",
       "         [ 70,  72],\n",
       "         [ 72,  73],\n",
       "         [ 73,  79],\n",
       "         [ 80,  85],\n",
       "         [ 85,  86],\n",
       "         [ 87,  94],\n",
       "         [ 95,  96],\n",
       "         [ 96,  98],\n",
       "         [ 98,  99],\n",
       "         [ 99, 101],\n",
       "         [101, 104],\n",
       "         [104, 108],\n",
       "         [108, 110],\n",
       "         [111, 114],\n",
       "         [115, 118],\n",
       "         [118, 120],\n",
       "         [120, 125],\n",
       "         [126, 127],\n",
       "         [127, 129],\n",
       "         [129, 132],\n",
       "         [132, 134],\n",
       "         [135, 137],\n",
       "         [137, 139],\n",
       "         [139, 141],\n",
       "         [141, 144],\n",
       "         [145, 146],\n",
       "         [147, 150],\n",
       "         [150, 155],\n",
       "         [156, 159],\n",
       "         [159, 162],\n",
       "         [162, 166],\n",
       "         [166, 169],\n",
       "         [170, 172],\n",
       "         [172, 173],\n",
       "         [173, 174],\n",
       "         [175, 176],\n",
       "         [177, 178],\n",
       "         [178, 181],\n",
       "         [181, 184],\n",
       "         [184, 185],\n",
       "         [186, 187],\n",
       "         [187, 189],\n",
       "         [189, 191],\n",
       "         [191, 192],\n",
       "         [193, 196],\n",
       "         [196, 199],\n",
       "         [199, 200],\n",
       "         [201, 204],\n",
       "         [204, 206],\n",
       "         [206, 208],\n",
       "         [208, 211],\n",
       "         [211, 215],\n",
       "         [215, 216],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1],\n",
       "         [ -1,  -1]])}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrg_dataset_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5ce585-dfe6-4ebc-abec-91b0c864b6fc",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a44117a-fe6a-4a1e-973c-857f4e70b060",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertModel, BertConfig, AutoConfig, AutoModel, DistilBertConfig\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, BertForSequenceClassification\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForTokenClassification, DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "99d13ed6-655c-4f07-b215-2848097259ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BertAgrrModel(transformers.PreTrainedModel):\n",
    "\n",
    "    def __init__(self, name='DeepPavlov/rubert-base-cased-sentence'):\n",
    "        super(BertAgrrModel, self).__init__(config=AutoConfig.from_pretrained(name, output_last_hidden_state=True))\n",
    "        #config = BertConfig.from_pretrained(\"distilbert-base-uncased\", output_last_hidden_state=True)\n",
    "        self.bert = AutoModel.from_pretrained(name)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.sentence_classifier = nn.Linear(1024, 1)\n",
    "        self.full_annotation_classifier = nn.Linear(1024, 6)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids, attention_mask)\n",
    "        \n",
    "        sequence_output = output.last_hidden_state\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        #print(sequence_output[:, :1, :].size())\n",
    "        pooled_output = sequence_output[:, 0, :]\n",
    "        #print(output.last_hidden_state.size(), pooled_output.size())\n",
    "        \n",
    "        #sequence_output, pooled_output = self.dropout(sequence_output), self.dropout(pooled_output)\n",
    "        #print(sequence_output[0])\n",
    "        \n",
    "        sentence_logits = self.sentence_classifier(pooled_output)\n",
    "        full_annotation_logits = self.full_annotation_classifier(sequence_output)\n",
    "        sentence_probs = torch.sigmoid(sentence_logits)\n",
    "        full_annotation_probs = torch.nn.functional.softmax(full_annotation_logits, dim=2)\n",
    "        \n",
    "        return {\n",
    "            'sentence_logits': sentence_logits,\n",
    "            'full_annotation_logits': full_annotation_logits,\n",
    "            'sentence_probs': sentence_probs,\n",
    "            'full_annotation_probs': full_annotation_probs\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "123c7299-3d4e-4c47-a0eb-21d1545b84bf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [157]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrg_dataset_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokens_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marrg_dataset_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [153]\u001b[0m, in \u001b[0;36mBertAgrrModel.forward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask):\n\u001b[0;32m---> 15\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     sequence_output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#print(sequence_output[:, :1, :].size())\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:566\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    563\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 566\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[1;32m    568\u001b[0m     x\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m    569\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    573\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    574\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:118\u001b[0m, in \u001b[0;36mEmbeddings.forward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    Parameters:\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m        input_ids: torch.tensor(bs, max_seq_length) The token ids to embed.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m    embeddings)\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     seq_length \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# Setting the position-ids to the registered buffer in constructor, it helps\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# when tracing the model without passing position-ids, solves\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# isues similar to issue #5664\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposition_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "out = model(arrg_dataset_train[1]['tokens_ids'], arrg_dataset_train[1]['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f69e0a1a-e075-4114-80d2-1956cc27a2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 31178,   117, 15127, 17835, 10124, 21610, 10112,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_tmp = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "inputs_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62984e-2386-4ec5-964a-5fd4b69427e3",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37c8091a-8227-48c8-b101-f7ce463f2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "426feba8-3eb3-4add-9869-bf0b7a098c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AdamW\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f58cc39c-c518-4a91-8ede-9f2c605e5420",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 10\n",
    "device = 'cuda:4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7b8de3d0-6798-4fc8-b61d-baab8bde28a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sberbank-ai/ruRoberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at sberbank-ai/ruRoberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertAgrrModel(NAME_MODEL).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3ecb25d6-80a6-471a-847f-a01b804ff084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "train_dataloader = DataLoader(arrg_dataset_train, shuffle=True, batch_size=24)\n",
    "test_dataloader = DataLoader(arrg_dataset_test, shuffle=False, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "20937153-b4d6-454b-89ce-24d6a62696f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "#num_training_steps = EPOCH * len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2a472726-c05e-4977-aba7-a01a6fbfc983",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1 Starting eval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:13<00:00, 38.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval metrics: {'f1_score': 0.39740119112073635, 'f1_symbolwise_score': 0}\n",
      "Step: 64, Loss: 0.43169857980683446\n",
      "Step: 128, Loss: 0.19843737816700013\n",
      "Step: 192, Loss: 0.11548110321746208\n",
      "Step: 256, Loss: 0.08184751464432338\n",
      "Step: 320, Loss: 0.07952318972820649\n",
      "Step: 384, Loss: 0.07970733606998692\n",
      "Step: 448, Loss: 0.07972718431119574\n",
      "Step: 512, Loss: 0.07541884650709108\n",
      "Step: 576, Loss: 0.06741264771699207\n",
      "Step: 640, Loss: 0.07584255089750513\n",
      "EPOCH: 2 Starting eval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:09<00:00, 52.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval metrics: {'f1_score': 0.9480337078651686, 'f1_symbolwise_score': 0}\n",
      "Step: 64, Loss: 0.047294343981775455\n",
      "Step: 128, Loss: 0.03343502437655843\n",
      "Step: 192, Loss: 0.03488314957030525\n",
      "Step: 256, Loss: 0.04183571723115165\n",
      "Step: 320, Loss: 0.03297141372695478\n",
      "Step: 384, Loss: 0.032480388439580565\n",
      "Step: 448, Loss: 0.032637910577250295\n",
      "Step: 512, Loss: 0.04767977667142986\n",
      "Step: 576, Loss: 0.0509710059104691\n",
      "Step: 640, Loss: 0.04645107939722948\n",
      "EPOCH: 3 Starting eval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:13<00:00, 38.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval metrics: {'f1_score': 0.9740932642487046, 'f1_symbolwise_score': 0}\n",
      "Step: 64, Loss: 0.018530450896832917\n",
      "Step: 128, Loss: 0.024552059210691368\n",
      "Step: 192, Loss: 0.022919905437447596\n",
      "Step: 256, Loss: 0.02443382360797841\n",
      "Step: 320, Loss: 0.018102619632372807\n",
      "Step: 384, Loss: 0.015974085863490473\n",
      "Step: 448, Loss: 0.03705477924995648\n",
      "Step: 512, Loss: 0.019514679586791317\n",
      "Step: 576, Loss: 0.015781521479766525\n",
      "Step: 640, Loss: 0.020294165648010676\n",
      "EPOCH: 4 Starting eval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:09<00:00, 52.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval metrics: {'f1_score': 0.9794117647058823, 'f1_symbolwise_score': 0}\n",
      "Step: 64, Loss: 0.00872164196380254\n",
      "Step: 128, Loss: 0.017134509334027825\n",
      "Step: 192, Loss: 0.01742696803421495\n",
      "Step: 256, Loss: 0.0109619172424118\n",
      "Step: 320, Loss: 0.018577032934899762\n",
      "Step: 384, Loss: 0.013888313113966433\n",
      "Step: 448, Loss: 0.018743955640729837\n",
      "Step: 512, Loss: 0.02044701982413244\n",
      "Step: 576, Loss: 0.013996967319599207\n",
      "Step: 640, Loss: 0.02046492751514961\n",
      "EPOCH: 5 Starting eval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:09<00:00, 52.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval metrics: {'f1_score': 0.9539333805811482, 'f1_symbolwise_score': 0}\n",
      "Step: 64, Loss: 0.010591029603801871\n",
      "Step: 128, Loss: 0.012947628257506949\n",
      "Step: 192, Loss: 0.014381381871203303\n",
      "Step: 256, Loss: 0.005677099461763646\n",
      "Step: 320, Loss: 0.015851959404471927\n",
      "Step: 384, Loss: 0.003235668186334806\n",
      "Step: 448, Loss: 0.01884469546780565\n",
      "Step: 512, Loss: 0.016279509540254367\n",
      "Step: 576, Loss: 0.021050382685189106\n",
      "Step: 640, Loss: 0.02316238005141713\n",
      "EPOCH: 6 Starting eval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:09<00:00, 52.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval metrics: {'f1_score': 0.9748520710059171, 'f1_symbolwise_score': 0}\n",
      "Step: 64, Loss: 0.004882376497675978\n",
      "Step: 128, Loss: 0.010391893971245736\n",
      "Step: 192, Loss: 0.002287986666942743\n",
      "Step: 256, Loss: 0.009528517511512291\n",
      "Step: 320, Loss: 0.021787045401879368\n",
      "Step: 384, Loss: 0.010240421092021279\n",
      "Step: 448, Loss: 0.012304847286713994\n",
      "Step: 512, Loss: 0.010450490365087717\n",
      "Step: 576, Loss: 0.016165025238137787\n",
      "Step: 640, Loss: 0.010709836605201417\n",
      "EPOCH: 7 Starting eval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:09<00:00, 52.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval metrics: {'f1_score': 0.9772893772893773, 'f1_symbolwise_score': 0}\n",
      "Step: 64, Loss: 0.0015368571311000778\n",
      "Step: 128, Loss: 0.008778041982054674\n",
      "Step: 192, Loss: 0.00596420283579846\n",
      "Step: 256, Loss: 0.010154633904335242\n",
      "Step: 320, Loss: 0.0023224200679123896\n",
      "Step: 384, Loss: 0.011636080278265126\n",
      "Step: 448, Loss: 0.007389838272729321\n",
      "Step: 512, Loss: 0.024929474849272992\n",
      "Step: 576, Loss: 0.023473969465158007\n",
      "Step: 640, Loss: 0.015975149884980056\n",
      "EPOCH: 8 Starting eval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:09<00:00, 52.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval metrics: {'f1_score': 0.9808541973490426, 'f1_symbolwise_score': 0}\n",
      "Step: 64, Loss: 0.0077903881995098345\n",
      "Step: 128, Loss: 0.009944026243829285\n",
      "Step: 192, Loss: 0.011886466596706668\n",
      "Step: 256, Loss: 0.027510089993484144\n",
      "Step: 320, Loss: 0.03388034669603712\n",
      "Step: 384, Loss: 0.22922325442596048\n",
      "Step: 448, Loss: 0.7217311272397637\n",
      "Step: 512, Loss: 0.6412523603066802\n",
      "Step: 576, Loss: 0.6585872732102871\n",
      "Step: 640, Loss: 0.6317782048135996\n",
      "EPOCH: 9 Starting eval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:09<00:00, 52.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval metrics: {'f1_score': 0.8474576271186441, 'f1_symbolwise_score': 0}\n",
      "Step: 64, Loss: 0.1581112303683767\n",
      "Step: 128, Loss: 0.09823296128888614\n",
      "Step: 192, Loss: 0.041417436754272785\n",
      "Step: 256, Loss: 0.03580755866460095\n",
      "Step: 320, Loss: 0.029602047197840875\n",
      "Step: 384, Loss: 0.017332134325442894\n",
      "Step: 448, Loss: 0.03483289979385518\n",
      "Step: 512, Loss: 0.17107464175205678\n",
      "Step: 576, Loss: 0.280030480469577\n",
      "Step: 640, Loss: 0.14356238367327023\n",
      "EPOCH: 10 Starting eval...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [00:09<00:00, 52.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval metrics: {'f1_score': 0.9774872912127814, 'f1_symbolwise_score': 0}\n",
      "Step: 64, Loss: 0.01685074011038523\n",
      "Step: 128, Loss: 0.01614201113989111\n",
      "Step: 192, Loss: 0.018197376672105747\n",
      "Step: 256, Loss: 0.034175646868789045\n",
      "Step: 320, Loss: 0.03508676506498887\n",
      "Step: 384, Loss: 0.024866544133146817\n",
      "Step: 448, Loss: 0.023352856333076488\n",
      "Step: 512, Loss: 0.017216839281445573\n",
      "Step: 576, Loss: 0.014935805354070908\n",
      "Step: 640, Loss: 0.016104615825042856\n"
     ]
    }
   ],
   "source": [
    "f1_scores = []\n",
    "all_loss = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCH):\n",
    "    num_batches = 0\n",
    "    losses = []\n",
    "    \n",
    "    model.eval()\n",
    "    print('EPOCH: {} Starting eval...'.format(epoch+1))\n",
    "    pred_labeles = []\n",
    "    logits = []\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        token_ids = batch['tokens_ids']\n",
    "        mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "        tags = batch['tags_ids']\n",
    "        #print(token_ids, mask)\n",
    "        out = model(token_ids.to(device), mask.to(device))\n",
    "\n",
    "        probs = out['sentence_probs'].detach().to('cpu').numpy().flatten()\n",
    "        logits += list(probs)\n",
    "        probs[probs >= 0.5] = 1\n",
    "        probs[probs < 0.5] = 0\n",
    "        pred_labeles += list(probs)\n",
    "\n",
    "    pred_df = pd.DataFrame({\n",
    "        'class': pred_labeles,\n",
    "        'cV': [0]*len(pred_labeles),\n",
    "        'cR1': [0]*len(pred_labeles),\n",
    "        'cR2': [0]*len(pred_labeles),\n",
    "        'R1': [0]*len(pred_labeles),\n",
    "        'R2': [0]*len(pred_labeles)\n",
    "    })\n",
    "    #print(logits)\n",
    "    #print(pred_df)\n",
    "    #print(test_df)\n",
    "\n",
    "    metrics = gapping_metrics(test_df, pred_df, only_class=True)\n",
    "    print('Eval metrics:', metrics)\n",
    "    f1_scores.append(metrics)\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        token_ids = batch['tokens_ids']\n",
    "        mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "        tags = batch['tags_ids']\n",
    "        \n",
    "        labels = labels.type(torch.float32).to(device)\n",
    "        out = model(token_ids.to(device), mask.to(device))\n",
    "        \n",
    "        #print(torch.squeeze(out['sentence_logits']))\n",
    "        loss = criterion(torch.squeeze(out['sentence_probs']), labels)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if (i+1) % 64 == 0:\n",
    "            print('Step: {}, Loss: {}'.format(i+1, np.mean(losses)))\n",
    "            all_loss.append(np.mean(losses))\n",
    "            losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a1b5cebd-81ae-43b3-a668-4cff72315935",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "808a89fc-10de-4e63-aa1a-90234c796c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics[NAME_MODEL] = {'loss': all_loss, 'scores': f1_scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0f218445-744b-4669-8732-1d70c6d85204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['distilbert-base-multilingual-cased', 'DeepPavlov/rubert-base-cased-sentence', 'bert-base-multilingual-uncased', 'sberbank-ai/ruRoberta-large'])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metrics.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f3171498-0dad-4908-aeb1-c0812be1bbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('all_metrics.pickle', 'wb') as f:\n",
    "    pickle.dump(all_metrics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "154b2fc9-7d3c-40b3-9cbf-ed4ededb347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics['deeppavlov_rubert'] = {'loss': all_loss, 'scores': f1_scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c4b4f796-78ce-41a2-b50d-d0724d0a4adc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float64' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgapping_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mgapping_metrics\u001b[0;34m(true_data, pred_data, only_class)\u001b[0m\n\u001b[1;32m     36\u001b[0m     f1_symbolwise_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcV\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcR1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcR2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR2\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m---> 38\u001b[0m         f1_symbolwise_scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [get_rank(true_data\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m], pred_data\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m], true_data\u001b[38;5;241m.\u001b[39miloc[i][tag], pred_data\u001b[38;5;241m.\u001b[39miloc[i][tag]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(true_data))]\n\u001b[1;32m     39\u001b[0m     f1_symbolwise_score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(f1_symbolwise_scores)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_score\u001b[39m\u001b[38;5;124m'\u001b[39m: f1_class, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_symbolwise_score\u001b[39m\u001b[38;5;124m'\u001b[39m: f1_symbolwise_score}\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     36\u001b[0m     f1_symbolwise_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcV\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcR1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcR2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR2\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m---> 38\u001b[0m         f1_symbolwise_scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[43mget_rank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(true_data))]\n\u001b[1;32m     39\u001b[0m     f1_symbolwise_score \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(f1_symbolwise_scores)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_score\u001b[39m\u001b[38;5;124m'\u001b[39m: f1_class, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_symbolwise_score\u001b[39m\u001b[38;5;124m'\u001b[39m: f1_symbolwise_score}\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mget_rank\u001b[0;34m(true_class, pred_class, true_span, pred_span)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_rank\u001b[39m(true_class, pred_class, true_span, pred_span):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m true_class \u001b[38;5;241m==\u001b[39m pred_class \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msymbol_wize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_span\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_span\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m true_class \u001b[38;5;241m==\u001b[39m pred_class \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36msymbol_wize\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msymbol_wize\u001b[39m(y_true, y_pred):\n\u001b[0;32m----> 4\u001b[0m     y_true1, y_pred1 \u001b[38;5;241m=\u001b[39m \u001b[43my_true\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m(), y_pred\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m      5\u001b[0m     y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(), \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m      7\u001b[0m     eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-7\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "gapping_metrics(pred_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "40828fc1-db14-4db7-b6e9-c19327566f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>cV</th>\n",
       "      <th>cR1</th>\n",
       "      <th>cR2</th>\n",
       "      <th>R1</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.238321</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.238321</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.238321</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.238321</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.238321</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040</th>\n",
       "      <td>0.238321</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2041</th>\n",
       "      <td>0.238321</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2042</th>\n",
       "      <td>0.238321</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043</th>\n",
       "      <td>0.238321</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044</th>\n",
       "      <td>0.238321</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2045 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         class  cV  cR1  cR2  R1  R2\n",
       "0     0.238321   0    0    0   0   0\n",
       "1     0.238321   0    0    0   0   0\n",
       "2     0.238321   0    0    0   0   0\n",
       "3     0.238321   0    0    0   0   0\n",
       "4     0.238321   0    0    0   0   0\n",
       "...        ...  ..  ...  ...  ..  ..\n",
       "2040  0.238321   0    0    0   0   0\n",
       "2041  0.238321   0    0    0   0   0\n",
       "2042  0.238321   0    0    0   0   0\n",
       "2043  0.238321   0    0    0   0   0\n",
       "2044  0.238321   0    0    0   0   0\n",
       "\n",
       "[2045 rows x 6 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8215ab26-903f-4347-a1c9-4ab1554f3efe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb8b367-816e-4a8e-9eb5-deaa2baf6eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a39395a2-2c9f-483c-8754-ef737422120e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Gapping'...\n",
      "remote: Enumerating objects: 115, done.\u001b[K\n",
      "remote: Total 115 (delta 0), reused 0 (delta 0), pack-reused 115\u001b[K\n",
      "Receiving objects: 100% (115/115), 13.20 MiB | 11.25 MiB/s, done.\n",
      "Resolving deltas: 100% (1/1), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/AlexeySorokin/Gapping.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d493b44f-eb9e-407c-a2f8-b68b822a86d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
